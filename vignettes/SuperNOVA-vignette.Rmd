---
title: "Analysis of Variance using Super Learner with Data-Adaptive Stochastic Interventions"
author: "David McCoy"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/references.bib
vignette: >
  %\VignetteIndexEntry{Analysis of Variance using Super Learner with Data-Adaptive Stochastic Interventions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Motivation 

The motivation behind the package SuperNOVA is to address the limitations of traditional statistical methods in environmental epidemiology studies. Analysts are often interested in understanding the joint impact of a mixed exposure, i.e. a vector of exposures, but the most important variables and variable sets remain unknown. Traditional methods make overly simplistic assumptions, such as linear and additive relationships, and the resulting statistical quantities may not be directly applicable to public policy decisions.

To overcome these limitations, SuperNOVA uses data-adaptive machine learning methods to identify the variables and variable sets that have the most explanatory power on an outcome of interest. The package builds a discrete Super Learner, which is then analyzed using ANOVA style analysis to determine the variables that contribute most to the model fit through basis functions. The target parameter, which may be a single shift, effect modification, or interaction, is then applied to the data-adaptively determined variable sets.

In this way, SuperNOVA allows analysts to explore modified treatment policies and ask causal questions, such as "If exposure to collections of PFAS chemicals increases, what is the change in cholesterol, immune function, or cancer?" By providing more nuanced and data-driven insights, SuperNOVA aims to inform public policy decisions and improve our understanding of the impact of mixed exposures on health outcomes.

### Data-Adaptive Machine Learning

The package SuperNOVA uses a data-adaptive approach to identify important variables and variable sets in mixed exposure studies. To avoid over-fitting and incorrect model assumptions, the package employs cross-validation procedures. In each fold, the training data is used to:

1. Identify the variable sets of interest using flexible machine learning algorithms.
2. Fit estimators for the relevant nuisance parameters and the final target parameter of interest using the identified variable sets.
3. Obtain estimates of the nuisance parameters and the final target parameter of interest using the held-out validation data.
4. The process is repeated in each fold to provide an estimate specific to each validation data. To optimize the balance between bias and variance, SuperNOVA uses targeted minimum loss based estimation (TMLE).
5. Estimates across the folds are then averaged and a pooled fluctuation step is performed to estimate the efficient influence function and derive pooled variance estimates.


## Data and Notation

### Our Data-Generating System 

Consider $n$ observed units $O_1, \ldots, O_n$, where each unit $O_i$ is a random variable $(V \subset W, A, Y)$. The variable $W$ represents baseline covariates (such as age, sex, and education level), which are not considered to be effect modifiers. $V$ is a subset of $W$ that may be potential effect modifiers. $A$ is the exposure variable of interest (e.g., mixed pesticide biomarkers, metals, etc.), and $Y$ is the outcome of interest (e.g., disease status).

We use a nonparametric structural equation model (NPSEM) to define how the system changes under interventions on $A$. The NPSEM is based on @pearl2000causality and the observed data is defined as $O = (W, A, Y)$.

The NPSEM assumes a temporal ordering of the variables, that $Y$ occurs after $A$, which occurs after $W$. Each variable is generated from its corresponding function, and each function depends on exogenous variables denoted by $U$. The exogenous variables contain all unobserved causes of the corresponding observed variable.

\begin{align*}\label{eqn:npsem}
  W &= f_W(U_W) \\ A &= f_A(W, U_A) \\ Y &= f_Y(A, W, U_Y),
\end{align*}

The likelihood of the data $O$ can be factorized, and the components of the likelihood are essential for understanding the impact of interventions and evaluating corresponding causal effects.

### Factorizing the Likelihood

The likelihood of the data $O$ admits a factorization, wherein, for $p_0^O$,
the density of $O$ with respect to the product measure, the density evaluated
on a particular observation $o$ may be a written
\begin{equation}
  p_0^O(o) = q^O_{0,Y}(y \mid A = a, W = w) q^O_{0,A}(a \mid W = w)
  q^O_{0,W}(w)
\end{equation}
where $q_{0, Y}$ is the conditional density of $Y$ given $(A, W)$ with respect
to some dominating measure, $q_{0, A}$ is the conditional density of $A$ given
$W$ with respect to dominating measure $\mu$, and $q_{0, W}$ is the density of
$W$ with respect to dominating measure $\nu$. Further, for ease of notation, let
$Q(A, W) = \mathbb{E}[Y \mid A, W]$, $g(A \mid W) = \mathbb{P}(A \mid W)$, and $q_W$ the
marginal distribution of $W$. These components of the likelihood will be
essential in developing an understanding of the manner in which stochastic
treatment regimes perturb a system and how a corresponding causal effect may be
evaluated. 


### Overview Stochastic Shifts

In single exposure problems, we can use the stochastic shift framework to calculate the average outcome after shifting the exposure. A stochastic intervention changes the function that defines the exposure $A$ and its conditional density $g(A \mid W)$ with a candidate density $g_{A_{\delta}}(A \mid W)$. The new density defines how the exposure is modified by a random draw from $g_{A_{\delta}}(A \mid W)$. This can include static interventions, where all mass is placed on a single value, such as the average treatment effect, where all observations either receive or don't receive treatment.

Stochastic interventions give rise to a counterfactual outcome $Y_{A_{\delta}} := f_Y(A_{\delta}, W, U_Y)$, which is obtained by replacing the natural value of the treatment with a shifted value. The degree of shift $\delta$ describes the reduction in exposure, based on the individual's baseline characteristics $W$. We can evaluate the causal effect of the intervention by finding the counterfactual mean of the outcome under the modified distribution, $\psi_{0, \delta} = E_{P_0}^{A_{\delta}}{Y_{A_{\delta}}}$. To accurately estimate this target causal estimand, we must assume that the data is generated by independent and identically distributed units and that there is no unmeasured confounding or interference and there is consistency.

Effectively, we estimate the the natural density of $a$ given $W$ and then get predicted values of this density under a shift, such as all individuals receive $10$ parts per trillion 
less exposure to endocrine disrupting compounds. We then estimate how this changes the health outcome under this shift. 

## Target Parameters

Of course, in the setting of mixed exposures we don't know which individual exposures have a marginal impact, which exposures have a joint impact and which marginal or joint impacts
may be modified by baseline covariates. Considering these variable(s) fixed for now, we develop non-parametric target parameters for interaction and effect modification based on stochastic interventions. Our target parameters are: 

1. Marginal impact of an exposure: $E[Y^{a + \delta}] - E[Y^a]$ - the expected outcome given a shift compared to expected outcome under observed exposure. 
2. Interaction of exposures: $\underbrace{E[E[Y_{A1_{\delta_1}, A2_{\delta_2}}]}_{\text{1. Joint Shift of A1 and A2}} - [\underbrace{E[Y_{A1_{\delta_1}, A2}]}_{\text{2. Individual Shift of A1}} + \underbrace{ E[Y_{A2_{\delta_2}, A1}]]]}_{\text{3. Individual Shift of A2}}$ - the expected outcome given a joint shift compared to the sum of individual shift of the respective variables
3. Effect Modification of an exposure: $$E[Y^{a + \delta} | V = 1]$$ - the expected outcome given a shift in an exposure within a covariate region

With these target parameters for a fixed exposure(s) defined we now can think about data-adaptively identifying them.

## Data-Adative Variable Set Selection

`SuperNOVA` fits a Super Learner for of estimators which use varying smoothness orders of basis functions and their tensor products. These include estimators such as `earth`, `polySpline` and `hal9001` which are all varying types of non-parametric regression functions. 

Once Super Learner has identified the best model we construct a model matrix of the basis functions (the linear combination of different regions in the observation space that are used to predict the outcome. We then partition the variance using classic ANOVA decompositions by testing the null that some set of coefficient values (the $\beta_{s,i}$) are 0 (e.g., all terms that contain a particular variable, or represent two-way basis functions). We can then aggregate partial F-statistics for the basis functions up to the variable level for variable/interaction importance measures. Algorithms used in `SuperNOVA` return tensor products of arbitrary order and include. 

### Basis functions

Indicator variables can be used to indicate if a variable $A$ is less than or equal to a specific value $a_s$. The same can be done for multiple variable, $A_1, A_2$, to determine if both variable are less than or equal to a specific value. Thus, a function of our outcome is written as: 

$$\psi_{\beta} = \beta_0 + \sum_{s\subset \{1,2,...,p\}}\sum_{i=1}^{n} \beta_{s,i} \phi_{s,i},
    \text{ where } \phi_{s,i} = I(\tilde{A}_{i,s} \leq w_s), A \in \mathbb{R}^p$$ 

and $s$ denotes indices of subsets of the $W$ (e.g., both functions of single variables and two variables). The placement of knot-points for every potential value is not feasible in most real-world scenarios and therefore `SuperNOVA` chooses the best estimator from a class of basis algorithms which approximate this exhaustive function.
    
### Defining Important Basis Functions

The user can pass into `SuperNOVA` the parameter `quantile_thresh` which designates the F-statistic threshold by which basis functions are kept. For example, if the user uses `quantile_thresh = 0.25` and the resulting vector is $M_1, M_1V_1, M_1M_3$ then this indicates that the basis functions that had an F-statistic above the 25th quantile included basis functions for $M_1$ alone, basis functions for $M_1$ and $V_1$ (exposure and effect modifier) and basis functions for $M_1$ and $M_3$ (twp exposures). 

This means that, within this fold we would train estimators for each of our estimates of interest, that is, the individual stochastic shift of $M_1$, how $V_1$ modifies the shift relationship between $M_1$ on $Y$, and the joint shift of $M1,M_3$. 

We mention folds above, this is because we use V-fold cross-validation wherein we split the full data into folds. In the parameter-generating sample, we find the variable sets from the best basis function estimator and we train our estimators for the nuisance parameters necessary for each parameter. In the estimation-sample, we then get our predictions from these models and our final plug-in targeted learning estimator for the fold specific estimates. This is explained in more detail next. 

## Data-Adaptive Esimation

Because the data is being used to both identify a target parameter and make estimates given this parameter, sample splitting (cross-validation) is used to avoid bias from over fitting, but still uses the entire data set to estimate a data-adaptive parameter. V-fold cross-validation involves: (i) ${1,..., n}$, observations, is divided into $V$ equal size subgroups, (ii) for each $v$, an estimation-sample, notationally $P_{n,v}$ , is defined by the v-th subgroup of size n/V, while the parameter-generating sample, $P_{n,v^c}$, is its complement. More concretely, for split $v$ the empirical distribution, $P_{n,v^c}$, is used to define the exposure(s) $A$ given our $\textit{a priori}$ basis function algorithm which outputs the target variables based on our ANOVA test of the basis functions used. The observations not in $P_{n,v^c}$, namely the empirical distribution for $P_{n,v}$ then is used to generate the parameter of interest. That is, if we had 1000 observations and 4 folds, our estimation sample $P_{n,v}$ would be of size 250. For each fold, the respective 750 observations would be used to train our estimators $g_n$ and $Q_n$ and given these estimators we then get predictions given the respective 250 observations. The predicted density, $g_n$ and outcome $Q_n$ are then used to construct our clever covariate for the shift intervention, $H_n(a_i, w_i)$ for the TMLE update within the fold. Therefore, TMLE updated estimates are given for each fold using the respective estimation sample data $P_{n,v}$. 

## Targeted Learning

We use targeted minimum loss based estimation (TMLE) to debias our initial outcome estimates given a shift in exposure such that the resulting estimator is asymptotically unbiased and has the smallest variance. This procedure involves constructing a least favorable submodel which uses a ratio of conditional exposure densities under shift and now shift as a covariate to debias the initial estimates. More details of targeted learning for stochastic shifts are here @diaz2012population


## Pooling Estimates Found Across the Folds

Because `SuperNOVA` gets the TMLE updated shift parameter for each condition (individual, effect modification interaction and joint), for each fold, we can pool estimates that are found across all folds (are consistently found in the data adaptive procedure). We do this by stacking the nuisance parameter estimates across the folds to do a pooled fluctuation step, making use of the full data to solve the efficient influence function. This allows us to estimate variance for the full data which results in tighter confidence intervals. Point estimates across the folds are averaged. Thus, `SuperNOVA` outputs estimates for both V-fold specific results and pooled estimates. 

## Data Adaptive Delta

For each exposure, the user inputs a respective delta, for example for exposures $M1, M2, M3$ the analyst puts in the `deltas` vector 

```{r deltas, message=FALSE, warning=FALSE}
deltas <- c("M1" = 1, "M2" = 2.3, "M3" = 1.4)
```

Which assigns a delta shift amount to each exposure. However, because the user doesn't know the underlying experimentation in the data, a delta that is too big may result in positivity violations which leads to bias and high variance of the estimator. That is, if the user asks for a shift in exposure density that is very unlikely given the data. To solve this, the analyst can also choose to set `adaptive_delta = TRUE` which then makes the shift amount a data adaptive parameter as well. The delta will be reduced until the max of the ratio of densities for each exposure is less than or equal to `hn_trunc_thresh`.

## Application:

First, let's load the packages we'll use and set a seed for simulation:

```{r setup, message=FALSE, warning=FALSE}
library(data.table)
library(dplyr)
library(kableExtra)
library(SuperNOVA)

seed <- 325911
```

Below we show implementation of `SuperNOVA` using simulated data. Because estimates are given for each data-adaptively identified parameter for each fold, after the demonstration we also show our pooled estimate approach for parameters that are found across all the folds. 

## NIEHS Mixture Workshop Data

The `SupernOVA` package comes with 2 datasets from the 2015-NIEHS-Mixtures-Workshop simulation data (https://github.com/niehs-prime/2015-NIEHS-MIxtures-Workshop). Let's load this data and run `SuperNOVA` to see if we identify 1. any interactions in the mixture variables or any marginally important mixture variables and/or 2. any effect modifying variables.

```{r NIEHS example}
data("NIEHS_data_1", package = "SuperNOVA")
```

```{r NIEH Nodes}
NIEHS_data_1$W <- rnorm(nrow(NIEHS_data_1), mean = 0, sd = 0.1)
w <- NIEHS_data_1[, c("W", "Z")]
a <- NIEHS_data_1[, c("X1", "X2", "X3", "X4", "X5", "X6", "X7")]
y <- NIEHS_data_1$Y
```


```{r run SuperNOVA NIEHS data, eval = TRUE}
deltas <- list(
  "X1" = 1, "X2" = 1, "X3" = 1,
  "X4" = 1, "X5" = 1, "X6" = 1, "X7" = 1
)

ptm <- proc.time()

NIEH_results <- SuperNOVA(
  w = w,
  a = a,
  y = y,
  deltas = deltas,
  estimator = "tmle",
  fluctuation = "standard",
  n_folds = 2,
  outcome_type = "continuous",
  quantile_thresh = 0,
  verbose = TRUE,
  parallel = TRUE,
  num_cores = 5,
  seed = seed,
  adaptive_delta = TRUE
)

proc.time() - ptm

indiv_shift_results <- NIEH_results$`Indiv Shift Results`
em_results <- NIEH_results$`Effect Mod Results`
joint_shift_results <- NIEH_results$`Joint Shift Results`
```


Let's look at the results for the variable $X7$ which should have a positive 
effect given the data dictionary provided: 

```{r individual results}
indiv_shift_results$X7 %>%
  kableExtra::kbl(caption = "Effect Modification Results") %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

We ran `SuperNOVA` with two folds and above we see this exposure was identified in both folds. We also set `adaptive_delta` = TRUE, 
with an initial shift value of 1. As we can see - in both folds this exposure was found, the delta was reduced to 0.59 to ensure there are no positivity violations (incurring a shift that is unfeasible). The pooled estimate is an average across the folds and variance estimates for the pooled result are done via a pooled targeted estimation fluctuation step using nuisance parameters across the folds. Here, our interpretation of this finding is "If all individuals were exposed to a 0.59 increase in X7 the expected outcome increases by 2.37". This result is significant for both the pooled estimate and the fold specific estimates. 

Now let's look for effect modification: 
```{r joint results}
em_results$X7Z %>%
  kableExtra::kbl(caption = "Effect Modification Results") %>%
  kableExtra::kable_classic(full_width = F, html_font = "Cambria")
```

This shows there are differential effects on the impact of shifting $X7$ between
strata of the baseline covariate Z. This result was found in 1 fold and so the fold specific and pooled results are the same. This is an example of a less consistent finding as it wasn't found in all the folds. The user should incorporate this information into their interpretation of findings. Findings are more consistent with higher CV fold values. 

Here we see that when Z = 0 a 0.47 shift in X7 leads to an average outcome of 17.43 and when Z = 1 the average outcome for the same shift is 33.7. 

```{r interaction results}
joint_shift_results$X2X7 %>%
  kableExtra::kbl(caption = "Interaction Results") %>%
  kableExtra::kable_classic(full_width = T, html_font = "Cambria")
```

Here we see that in one of the folds an interaction between X2 and X7 was found. The adaptive delta was set to 0.22 for X2 and 0.35 for X7. An increase of 0.22 in X2 leads to a 0.64 increase in Y and a 0.35 increase in X7 leads to a 1.23 increase in Y. An increase in both by these amounts changes Y by 1.87 compared to the additive sum 1.87 if we just add both these individual estimates together. Thus, there is no evidence of interaction given our definition in this case that is more than additive effects. 





